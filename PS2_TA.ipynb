{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QJ70F7u90d56"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PFC6bCV0gTi",
        "outputId": "d9b11bb4-4fbf-4677-e2ff-8605d46319b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from IPython.display import display, HTML, Markdown"
      ],
      "metadata": {
        "id": "nfyQwTOy3V6r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_attack_data():\n",
        "    \"\"\"Load MITRE ATT&CK data from the MITRE STIX repository or local cache\"\"\"\n",
        "    attack_data_path = \"attack_data.csv\"\n",
        "\n",
        "    if os.path.exists(attack_data_path):\n",
        "        return pd.read_csv(attack_data_path)\n",
        "\n",
        "    print(\"Downloading MITRE ATT&CK data...\")\n",
        "    # If not cached, fetch from MITRE ATT&CK API\n",
        "    url = \"https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract techniques and tactics\n",
        "    techniques = []\n",
        "    for obj in data[\"objects\"]:\n",
        "        if obj.get(\"type\") == \"attack-pattern\":\n",
        "            techniques.append({\n",
        "                \"technique_id\": obj.get(\"external_references\", [{}])[0].get(\"external_id\", \"\"),\n",
        "                \"technique_name\": obj.get(\"name\", \"\"),\n",
        "                \"description\": obj.get(\"description\", \"\"),\n",
        "                \"tactic_phase\": \", \".join([p.get(\"phase_name\", \"\") for p in obj.get(\"kill_chain_phases\", [])])\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(techniques)\n",
        "    df.to_csv(attack_data_path, index=False)\n",
        "    print(f\"Saved {len(techniques)} techniques to {attack_data_path}\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "vsXBf5IH5Exg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_llm_model(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
        "    \"\"\"Load the specified LLM model\"\"\"\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(\"Model loaded successfully\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "mN-n_fdf5HuS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_attack_info(text, model, tokenizer):\n",
        "    \"\"\"Extract ATT&CK tactics and techniques from text using LLM\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are a cybersecurity expert specializing in MITRE ATT&CK framework. Analyze the following cyber threat intelligence text and extract all ATT&CK tactics and techniques mentioned explicitly or implicitly.\n",
        "\n",
        "    For each identified technique, provide:\n",
        "    1. The ATT&CK Technique ID (e.g., T1566)\n",
        "    2. The technique name (e.g., Phishing)\n",
        "    3. The tactic category (e.g., Initial Access)\n",
        "    4. A brief explanation of why this technique was identified in the text\n",
        "    5. The specific text excerpt that indicates this technique (if available)\n",
        "\n",
        "    Format your response as JSON with the structure:\n",
        "    {{\n",
        "        \"techniques\": [\n",
        "            {{\n",
        "                \"technique_id\": \"T1566\",\n",
        "                \"technique_name\": \"Phishing\",\n",
        "                \"tactic\": \"Initial Access\",\n",
        "                \"explanation\": \"The report mentions phishing emails containing malicious attachments\",\n",
        "                \"text_evidence\": \"Attackers sent targeted emails with PDF attachments containing exploits\"\n",
        "            }}\n",
        "        ]\n",
        "    }}\n",
        "\n",
        "    Text to analyze:\n",
        "    {text}\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Analyzing text with the LLM model...\")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=1024,\n",
        "            temperature=0.1,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the JSON part from the response\n",
        "    try:\n",
        "        json_match = re.search(r'(\\{.*\\})', response, re.DOTALL)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1)\n",
        "            # Clean up any extra characters\n",
        "            json_str = re.sub(r'```json|```', '', json_str).strip()\n",
        "            return json_str\n",
        "        else:\n",
        "            # If no JSON found, return the full response\n",
        "            return response\n",
        "    except Exception as e:\n",
        "        return f\"Error parsing response: {e}\\n\\nFull response: {response}\""
      ],
      "metadata": {
        "id": "xjozvTB-5NEo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_large_document(document, model, tokenizer, chunk_size=2000, overlap=200):\n",
        "    \"\"\"Split large documents into manageable chunks and process each\"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(document):\n",
        "        end = min(start + chunk_size, len(document))\n",
        "        # Try to find a good breaking point (end of sentence)\n",
        "        if end < len(document):\n",
        "            # Look for period, question mark, or exclamation followed by space or newline\n",
        "            match = re.search(r'[.!?][\\s\\n]', document[end-100:end])\n",
        "            if match:\n",
        "                end = end - 100 + match.end()\n",
        "\n",
        "        chunks.append(document[start:end])\n",
        "        start = end - overlap\n",
        "\n",
        "    print(f\"Processing document in {len(chunks)} chunks...\")\n",
        "    all_results = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
        "        result = extract_attack_info(chunk, model, tokenizer)\n",
        "        all_results.append(result)\n",
        "\n",
        "    # Combine results from different chunks\n",
        "    combined_techniques = []\n",
        "    seen_ids = set()\n",
        "\n",
        "    for result in all_results:\n",
        "        try:\n",
        "            data = json.loads(result)\n",
        "            if \"techniques\" in data:\n",
        "                for technique in data[\"techniques\"]:\n",
        "                    tech_id = technique.get(\"technique_id\")\n",
        "                    if tech_id and tech_id not in seen_ids:\n",
        "                        combined_techniques.append(technique)\n",
        "                        seen_ids.add(tech_id)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "    return json.dumps({\"techniques\": combined_techniques})"
      ],
      "metadata": {
        "id": "zgpUnOL_5QXA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(json_result):\n",
        "    \"\"\"Format and display the extracted techniques in a readable way\"\"\"\n",
        "    try:\n",
        "        data = json.loads(json_result)\n",
        "\n",
        "        if \"techniques\" not in data or not data[\"techniques\"]:\n",
        "            display(Markdown(\"### No ATT&CK techniques identified in the text.\"))\n",
        "            return\n",
        "\n",
        "        html = \"<div style='background-color:#f8f9fa; padding:15px; border-radius:5px;'>\"\n",
        "        html += \"<h2>Extracted ATT&CK Tactics and Techniques</h2>\"\n",
        "\n",
        "        for technique in data[\"techniques\"]:\n",
        "            html += f\"<div style='margin-bottom:20px; padding:10px; border-left:4px solid #007bff;'>\"\n",
        "            html += f\"<h3>{technique.get('technique_id', 'Unknown ID')} - {technique.get('technique_name', 'Unknown Technique')}</h3>\"\n",
        "            html += f\"<p><strong>Tactic:</strong> {technique.get('tactic', 'Unknown')}</p>\"\n",
        "            html += f\"<p><strong>Explanation:</strong> {technique.get('explanation', 'No explanation provided')}</p>\"\n",
        "            html += f\"<p><strong>Evidence:</strong><br><em>{technique.get('text_evidence', 'No specific evidence cited')}</em></p>\"\n",
        "            html += \"</div>\"\n",
        "\n",
        "        html += \"</div>\"\n",
        "        display(HTML(html))\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Failed to parse results as JSON. Raw output:\")\n",
        "        print(json_result)"
      ],
      "metadata": {
        "id": "hjvhXoLj5T8G"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_threat_report(threat_report, model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
        "    \"\"\"Main function to analyze a threat report and extract ATT&CK information\"\"\"\n",
        "    # Load model\n",
        "    model, tokenizer = load_llm_model(model_name)\n",
        "    if model is None or tokenizer is None:\n",
        "        print(\"Failed to load model. Please try a different model.\")\n",
        "        return\n",
        "\n",
        "    # Process report\n",
        "    if len(threat_report) > 2000:\n",
        "        print(\"Large document detected. Splitting into chunks for processing...\")\n",
        "        result = process_large_document(threat_report, model, tokenizer)\n",
        "    else:\n",
        "        result = extract_attack_info(threat_report, model, tokenizer)\n",
        "\n",
        "    # Display results\n",
        "    display_results(result)\n",
        "\n",
        "    # Return raw result\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "j5DQM7f85X7Z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_data = load_attack_data()\n",
        "\n",
        "# Sample threat report\n",
        "sample_report = \"\"\"\n",
        "APT29, also known as Cozy Bear, has been observed using spear-phishing emails with malicious attachments\n",
        "to gain initial access to victim networks. After gaining access, the threat actor deployed custom malware\n",
        "that uses DNS tunneling for command and control communications, effectively bypassing traditional\n",
        "network security controls. The attackers moved laterally through the network using stolen credentials\n",
        "and exploited Windows Management Instrumentation (WMI) to execute code remotely. They also modified\n",
        "registry keys to establish persistence and scheduled tasks to maintain access.\n",
        "Data was collected and compressed before being exfiltrated through encrypted channels.\n",
        "\"\"\"\n",
        "\n",
        "# Analyze the report\n",
        "result = analyze_threat_report(sample_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "Fxk9kwqG5Y9f",
        "outputId": "9a972366-7eb8-455d-a9cc-4dfe8713ab0d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading MITRE ATT&CK data...\n",
            "Saved 799 techniques to attack_data.csv\n",
            "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "Model loaded successfully\n",
            "Analyzing text with the LLM model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='background-color:#f8f9fa; padding:15px; border-radius:5px;'><h2>Extracted ATT&CK Tactics and Techniques</h2><div style='margin-bottom:20px; padding:10px; border-left:4px solid #007bff;'><h3>T1566 - Phishing</h3><p><strong>Tactic:</strong> Initial Access</p><p><strong>Explanation:</strong> The report mentions phishing emails containing malicious attachments</p><p><strong>Evidence:</strong><br><em>Attackers sent targeted emails with PDF attachments containing exploits</em></p></div></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}